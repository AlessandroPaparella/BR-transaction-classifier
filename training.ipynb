{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "training.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "nGujYpPPyoUe"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A-QHz5i50lsn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b431c7ab-0663-4ee1-9acf-e129bb6f7579"
      },
      "source": [
        "#Get access to gdrive space to import datasets...\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rpWz9ivS1Ohd"
      },
      "source": [
        "#Import training and validation datasets obtained with previous split (with preprocessing.py)\n",
        "!cp drive/MyDrive/training.csv ./training.csv\n",
        "!cp drive/MyDrive/validation.csv ./validation.csv\n",
        "\n",
        "TRAINING_LEN = 738390\n",
        "VALIDATION_LEN = 82044"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uEIuub3W1tfK"
      },
      "source": [
        "#Dataset to 3d tensor converion ([Samples, Time, Features])\n",
        "#Sample: Just TX row\n",
        "#Time: Sequence of the Tx trace\n",
        "#Feature: value, type etc... of the single calls\n",
        "\n",
        "import json, ast, textwrap\n",
        "import numpy as np\n",
        "\n",
        "#Features len\n",
        "TYPE_LEN=1\n",
        "INPUT_LEN=32\n",
        "OUTPUT_LEN=32\n",
        "GAS_LEN=1\n",
        "GASUSED_LEN=1\n",
        "CALLS_LEN=1\n",
        "\n",
        "TOTAL_LEN=TYPE_LEN+INPUT_LEN+OUTPUT_LEN+GAS_LEN+GASUSED_LEN+CALLS_LEN\n",
        "\n",
        "#Max calls per sample\n",
        "MAX_CALLS=150\n",
        "\n",
        "calls = {\n",
        "    \"CALL\": 1,\n",
        "    \"STATICCALL\": 2,\n",
        "    \"DELEGATECALL\": 3,\n",
        "    \"CALLCODE\": 4,\n",
        "    \"CREATE\": 5,\n",
        "    \"SELFDESTRUCT\": 6\n",
        "}\n",
        "\n",
        "#Horizontal padding for time dimension\n",
        "def pad():\n",
        "  p=[]\n",
        "  for i in range(68):\n",
        "    p.append(0)\n",
        "  return p\n",
        "\n",
        "#Split a large hex number \"n\" (string format) into \"p\" 64bit token\n",
        "def split(n, p):\n",
        "  tokens = textwrap.wrap(n, 16)\n",
        "  for i in range(len(tokens)):\n",
        "    tokens[i]=str(\"0x\"+tokens[i])\n",
        "    tokens[i] = int(tokens[i], 0) % 2 ** 64\n",
        "    if i==p:\n",
        "      break\n",
        "  #pad to passed length \"p\"\n",
        "  if len(tokens)<p:\n",
        "    m = p-len(tokens)\n",
        "    for i in range(m):\n",
        "      tokens.append(0)\n",
        "  return tokens[:p]\n",
        "\n",
        "#Join array \"tok\" into \"row\"\n",
        "def insertInRow(tok, row):\n",
        "  for t in tok:\n",
        "    row.append(t)\n",
        "  return row\n",
        "\n",
        "total_data = []\n",
        "\n",
        "#Explore trace with DFS and put all calls into a 2d matrix (time x calls)\n",
        "def DFS(df):\n",
        "    global total_data\n",
        "    t = []\n",
        "    t.append(calls[df['type']])\n",
        "    t.append(int(df['gas'], 0) if \"gas\" in df else 0)\n",
        "    tok = split(df['input'][2:], INPUT_LEN) if \"input\" in df else split(\"0\", INPUT_LEN)\n",
        "    t = insertInRow(tok, t)\n",
        "    tok = split(df['output'][2:], OUTPUT_LEN) if \"output\" in df else split(\"0\", OUTPUT_LEN)\n",
        "    t = insertInRow(tok, t)\n",
        "    t.append(int(df['gasUsed'], 0) if \"output\" in df else 0)\n",
        "    #Bool flag that report if there are other nested calls or not\n",
        "    t.append(1 if \"calls\" in df else 0)\n",
        "    total_data.append(t)\n",
        "    if \"calls\" in df:\n",
        "        for d in df[\"calls\"]:\n",
        "          DFS(d)\n",
        "\n",
        "\n",
        "def calls_to_tensor(df):\n",
        "  examples=[]\n",
        "  for d in df.itertuples():\n",
        "    #Get txTrace column into a tree\n",
        "    txTrace=ast.literal_eval(d[1])\n",
        "    global total_data\n",
        "    total_data=[]\n",
        "    DFS(txTrace)\n",
        "    #Pad to MAX CALLS\n",
        "    i=len(total_data)\n",
        "    while i<150:\n",
        "      total_data.append(pad())\n",
        "      i+=1\n",
        "    examples.append(total_data[:150])\n",
        "  return tf.convert_to_tensor(np.array(examples), dtype=tf.uint64)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a1H-fPCt9Wer"
      },
      "source": [
        "#Load data for training in batch mode\n",
        "\n",
        "def load_data(Train_df,idx,\n",
        "              batch_size):\n",
        "    df = pd.read_csv(\n",
        "                  Train_df, skiprows=idx*batch_size,\n",
        "                  nrows=batch_size)\n",
        "    df.columns = ['txTrace', 'Label0', 'Label1']\n",
        "    x = calls_to_tensor(df)\n",
        "    y=tf.convert_to_tensor(df['Label0'], dtype=tf.uint64)\n",
        "    return (x, y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q5CIzPfJ9eBo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "690456b9-f5e8-4b11-c371-08906d00a578"
      },
      "source": [
        "#Connect to cluster TPU and get strategy distribution\n",
        "\n",
        "import os\n",
        "resolver = tf.distribute.cluster_resolver.TPUClusterResolver('grpc://' + os.environ['COLAB_TPU_ADDR'])\n",
        "tf.config.experimental_connect_to_cluster(resolver)\n",
        "tf.tpu.experimental.initialize_tpu_system(resolver)\n",
        "strategy = tf.distribute.experimental.TPUStrategy(resolver) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Clearing out eager caches\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Clearing out eager caches\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Initializing the TPU system: grpc://10.26.62.146:8470\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Initializing the TPU system: grpc://10.26.62.146:8470\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Finished initializing TPU system.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Finished initializing TPU system.\n",
            "WARNING:absl:`tf.distribute.experimental.TPUStrategy` is deprecated, please use  the non experimental symbol `tf.distribute.TPUStrategy` instead.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Found TPU system:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Found TPU system:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores: 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores: 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Num TPU Workers: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Num TPU Workers: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q1qu73_w9tTX"
      },
      "source": [
        "#Define model for classification\n",
        "\n",
        "from keras import Sequential\n",
        "from tensorflow.keras.layers import Dense, LSTM, Input\n",
        "\n",
        "def create_model():\n",
        "  model = Sequential()\n",
        "  model.add(Input((150,TOTAL_LEN)))\n",
        "  model.add(LSTM(units=4096))\n",
        "  model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JrfX2u9c9tnY"
      },
      "source": [
        "#Create model and compile\n",
        "with strategy.scope():\n",
        "    classification_model = create_model()\n",
        "    classification_model.compile(loss='binary_crossentropy', optimizer=tf.keras.optimizers.Adam(learning_rate=3e-5, epsilon=1e-08, clipnorm=1.0), metrics=[tf.keras.metrics.AUC()])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RwHfsaSQ9wqW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 417
        },
        "outputId": "3b033aa3-8c38-46e8-c59e-b0d963a116b5"
      },
      "source": [
        "import shutil\n",
        "#Training\n",
        "EPOCHS = 6\n",
        "BATCH_SIZE = 2048\n",
        "\n",
        "steps_per_epoch=np.ceil(TRAINING_LEN/BATCH_SIZE)\n",
        "validation_steps=np.ceil(VALIDATION_LEN/BATCH_SIZE)\n",
        "\n",
        "t_loss = 0\n",
        "t_auc = 0\n",
        "for e in range(EPOCHS):\n",
        "  print(\"Epoch \"+str(e+1))\n",
        "  for i in range(int(steps_per_epoch)):\n",
        "    train, y_train = load_data(\"training.csv\", i, BATCH_SIZE)\n",
        "    total_data = []\n",
        "    loss, auc = classification_model.train_on_batch(train, y_train)\n",
        "    t_loss+=loss\n",
        "    t_auc+=auc\n",
        "    print(\"\\rLoss: \"+str(t_loss/(i+1))+\" Auc: \"+str(t_auc/(i+1))+\" Steps: \"+str(i)+\"/\"+str(steps_per_epoch),end=' ')\n",
        "  #Save weights when epoch ends and backup on gdrive space, e.g: class_epoch_1.h5 etc... \n",
        "  file_name = \"./class_epoch_\"+str(e+1)+\".h5\"\n",
        "  classification_model.save_weights(file_name)\n",
        "  shutil.copy(\"/content/\"+file_name, \"drive/MyDrive/\"+file_name)\n",
        "  #Perform validation\n",
        "  results = []\n",
        "  for i in range(int(validation_steps)):\n",
        "    test, y_test = load_data(\"validation.csv\", i, BATCH_SIZE)\n",
        "    total_data = []\n",
        "    loss, auc = classification_model.test_on_batch(test, y_test)\n",
        "    results.append((loss, auc))\n",
        "  val_loss = 0\n",
        "  val_auc = 0\n",
        "  for i in range(len(results)):\n",
        "    val_loss+=results[i][0]\n",
        "    val_auc +=results[i][1]\n",
        "  val_loss=val_loss/len(results)\n",
        "  val_auc=val_auc/len(results)\n",
        "  print(\"val_loss: \"+str(val_loss)+\" val_auc: \"+str(val_auc))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "Loss: 0.22058600022191815 Auc: 0.937804401828972 Steps: 360/361.0 val_loss: 0.1930996019665788 val_auc: 0.9642154792459999\n",
            "Epoch 2\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-7c16a6334b31>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Epoch \"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"training.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0mtotal_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassification_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-075f30a38892>\u001b[0m in \u001b[0;36mload_data\u001b[0;34m(Train_df, idx, batch_size)\u001b[0m\n\u001b[1;32m      7\u001b[0m                   nrows=batch_size)\n\u001b[1;32m      8\u001b[0m     \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'txTrace'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Label0'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Label1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalls_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Label0'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muint64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-17-3c40c0461a21>\u001b[0m in \u001b[0;36mcalls_to_tensor\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m     93\u001b[0m       \u001b[0mi\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0mexamples\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m150\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexamples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muint64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8QFnN16fBgtn"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}