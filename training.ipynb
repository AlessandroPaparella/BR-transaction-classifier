{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "training.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AlessandroPaparella/BR-transaction-classifier/blob/main/training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nGujYpPPyoUe"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A-QHz5i50lsn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b431c7ab-0663-4ee1-9acf-e129bb6f7579"
      },
      "source": [
        "#Get access to gdrive space to import datasets...\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rpWz9ivS1Ohd"
      },
      "source": [
        "#Import training and validation datasets obtained with previous split (with preprocessing.py)\n",
        "!cp drive/MyDrive/training.csv ./training.csv\n",
        "!cp drive/MyDrive/validation.csv ./validation.csv\n",
        "\n",
        "TRAINING_LEN = 738390\n",
        "VALIDATION_LEN = 82044"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uEIuub3W1tfK"
      },
      "source": [
        "#Dataset to 3d tensor converion ([Samples, Time, Features])\n",
        "#Sample: Just TX row\n",
        "#Time: Sequence of the Tx trace\n",
        "#Feature: value, type etc... of the single calls\n",
        "\n",
        "import json, ast, textwrap\n",
        "import numpy as np\n",
        "\n",
        "#Features len\n",
        "TYPE_LEN=1\n",
        "INPUT_LEN=32\n",
        "OUTPUT_LEN=32\n",
        "GAS_LEN=1\n",
        "GASUSED_LEN=1\n",
        "CALLS_LEN=1\n",
        "\n",
        "TOTAL_LEN=TYPE_LEN+INPUT_LEN+OUTPUT_LEN+GAS_LEN+GASUSED_LEN+CALLS_LEN\n",
        "\n",
        "#Max calls per sample\n",
        "MAX_CALLS=150\n",
        "\n",
        "calls = {\n",
        "    \"CALL\": 1,\n",
        "    \"STATICCALL\": 2,\n",
        "    \"DELEGATECALL\": 3,\n",
        "    \"CALLCODE\": 4,\n",
        "    \"CREATE\": 5,\n",
        "    \"SELFDESTRUCT\": 6\n",
        "}\n",
        "\n",
        "#Horizontal padding for time dimension\n",
        "def pad():\n",
        "  p=[]\n",
        "  for i in range(68):\n",
        "    p.append(0)\n",
        "  return p\n",
        "\n",
        "#Split a large hex number \"n\" (string format) into \"p\" 64bit token\n",
        "def split(n, p):\n",
        "  tokens = textwrap.wrap(n, 16)\n",
        "  for i in range(len(tokens)):\n",
        "    tokens[i]=str(\"0x\"+tokens[i])\n",
        "    tokens[i] = int(tokens[i], 0) % 2 ** 64\n",
        "    if i==p:\n",
        "      break\n",
        "  #pad to passed length \"p\"\n",
        "  if len(tokens)<p:\n",
        "    m = p-len(tokens)\n",
        "    for i in range(m):\n",
        "      tokens.append(0)\n",
        "  return tokens[:p]\n",
        "\n",
        "#Join array \"tok\" into \"row\"\n",
        "def insertInRow(tok, row):\n",
        "  for t in tok:\n",
        "    row.append(t)\n",
        "  return row\n",
        "\n",
        "total_data = []\n",
        "\n",
        "#Explore trace with DFS and put all calls into a 2d matrix (time x calls)\n",
        "def DFS(df):\n",
        "    global total_data\n",
        "    t = []\n",
        "    t.append(calls[df['type']])\n",
        "    t.append(int(df['gas'], 0) if \"gas\" in df else 0)\n",
        "    tok = split(df['input'][2:], INPUT_LEN) if \"input\" in df else split(\"0\", INPUT_LEN)\n",
        "    t = insertInRow(tok, t)\n",
        "    tok = split(df['output'][2:], OUTPUT_LEN) if \"output\" in df else split(\"0\", OUTPUT_LEN)\n",
        "    t = insertInRow(tok, t)\n",
        "    t.append(int(df['gasUsed'], 0) if \"output\" in df else 0)\n",
        "    #Bool flag that report if there are other nested calls or not\n",
        "    t.append(1 if \"calls\" in df else 0)\n",
        "    total_data.append(t)\n",
        "    if \"calls\" in df:\n",
        "        for d in df[\"calls\"]:\n",
        "          DFS(d)\n",
        "\n",
        "\n",
        "def calls_to_tensor(df):\n",
        "  examples=[]\n",
        "  for d in df.itertuples():\n",
        "    #Get txTrace column into a tree\n",
        "    txTrace=ast.literal_eval(d[1])\n",
        "    global total_data\n",
        "    total_data=[]\n",
        "    DFS(txTrace)\n",
        "    #Pad to MAX CALLS\n",
        "    i=len(total_data)\n",
        "    while i<150:\n",
        "      total_data.append(pad())\n",
        "      i+=1\n",
        "    examples.append(total_data[:150])\n",
        "  return tf.convert_to_tensor(np.array(examples), dtype=tf.uint64)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a1H-fPCt9Wer"
      },
      "source": [
        "#Load data for training in batch mode\n",
        "\n",
        "def load_data(Train_df,idx,\n",
        "              batch_size):\n",
        "    df = pd.read_csv(\n",
        "                  Train_df, skiprows=idx*batch_size,\n",
        "                  nrows=batch_size)\n",
        "    df.columns = ['txTrace', 'Label0', 'Label1']\n",
        "    x = calls_to_tensor(df)\n",
        "    y=tf.convert_to_tensor(df['Label0'], dtype=tf.uint64)\n",
        "    return (x, y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q5CIzPfJ9eBo"
      },
      "source": [
        "#Connect to cluster TPU and get strategy distribution\n",
        "\n",
        "import os\n",
        "resolver = tf.distribute.cluster_resolver.TPUClusterResolver('grpc://' + os.environ['COLAB_TPU_ADDR'])\n",
        "tf.config.experimental_connect_to_cluster(resolver)\n",
        "tf.tpu.experimental.initialize_tpu_system(resolver)\n",
        "strategy = tf.distribute.experimental.TPUStrategy(resolver) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q1qu73_w9tTX"
      },
      "source": [
        "#Define model for classification\n",
        "\n",
        "from keras import Sequential\n",
        "from tensorflow.keras.layers import Dense, LSTM, Input\n",
        "\n",
        "def create_model():\n",
        "  model = Sequential()\n",
        "  model.add(Input((150,TOTAL_LEN)))\n",
        "  model.add(LSTM(units=4096))\n",
        "  model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JrfX2u9c9tnY"
      },
      "source": [
        "#Create model and compile\n",
        "with strategy.scope():\n",
        "    classification_model = create_model()\n",
        "    classification_model.compile(loss='binary_crossentropy', optimizer=tf.keras.optimizers.Adam(learning_rate=3e-5, epsilon=1e-08, clipnorm=1.0), metrics=[tf.keras.metrics.AUC()])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RwHfsaSQ9wqW"
      },
      "source": [
        "import shutil\n",
        "#Training\n",
        "EPOCHS = 6\n",
        "BATCH_SIZE = 2048\n",
        "\n",
        "steps_per_epoch=np.ceil(TRAINING_LEN/BATCH_SIZE)\n",
        "validation_steps=np.ceil(VALIDATION_LEN/BATCH_SIZE)\n",
        "\n",
        "t_loss = 0\n",
        "t_auc = 0\n",
        "for e in range(EPOCHS):\n",
        "  print(\"Epoch \"+str(e+1))\n",
        "  for i in range(int(steps_per_epoch)):\n",
        "    train, y_train = load_data(\"training.csv\", i, BATCH_SIZE)\n",
        "    total_data = []\n",
        "    loss, auc = classification_model.train_on_batch(train, y_train)\n",
        "    t_loss+=loss\n",
        "    t_auc+=auc\n",
        "    print(\"\\rLoss: \"+str(t_loss/(i+1))+\" Auc: \"+str(t_auc/(i+1))+\" Steps: \"+str(i)+\"/\"+str(steps_per_epoch),end=' ')\n",
        "  #Save weights when epoch ends and backup on gdrive space, e.g: class_epoch_1.h5 etc... \n",
        "  file_name = \"./class_epoch_\"+str(e+1)+\".h5\"\n",
        "  classification_model.save_weights(file_name)\n",
        "  shutil.copy(\"/content/\"+file_name, \"drive/MyDrive/\"+file_name)\n",
        "  #Perform validation\n",
        "  results = []\n",
        "  for i in range(int(validation_steps)):\n",
        "    test, y_test = load_data(\"validation.csv\", i, BATCH_SIZE)\n",
        "    total_data = []\n",
        "    loss, auc = classification_model.test_on_batch(test, y_test)\n",
        "    results.append((loss, auc))\n",
        "  val_loss = 0\n",
        "  val_auc = 0\n",
        "  for i in range(len(results)):\n",
        "    val_loss+=results[i][0]\n",
        "    val_auc +=results[i][1]\n",
        "  val_loss=val_loss/len(results)\n",
        "  val_auc=val_auc/len(results)\n",
        "  print(\"val_loss: \"+str(val_loss)+\" val_auc: \"+str(val_auc))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}